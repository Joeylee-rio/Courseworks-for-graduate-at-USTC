{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "os.chdir('/data2/home/zhaoyi/labs/USTC-labs/deeplearn_lab2')\n",
    "import torch\n",
    "import torchtext\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchtext.legacy import data, datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(lower=True, batch_first=True, fix_length=50)\n",
    "LABEL = data.Field(sequential=False)\n",
    "train, dev, test = data.TabularDataset.splits(path='/data2/home/zhaoyi/labs/USTC-labs/deeplearn_lab2/dataset/procd/', train='train.csv', validation='dev.csv', test='test.csv', \n",
    "                format='csv', fields=[('Text',TEXT),('Label',LABEL)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://zhuanlan.zhihu.com/p/447309785  word2vec approaches\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/562237953 pretrained word2vecs (e.g. glove of different versions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 399999/400000 [00:15<00:00, 25092.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# construct and load word-vectors from a pretrained file\n",
    "TEXT.build_vocab(train, vectors=\"glove.6B.100d\", max_size=10000, min_freq=10)\n",
    "# glove-file-location : workspace/.vector_cache\n",
    "LABEL.build_vocab(train)\n",
    "# print(TEXT.vocab.freqs.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 243059), ('a', 120429), ('and', 119709), ('of', 108726), ('to', 100984), ('is', 78572), ('in', 68301), ('i', 52918), ('this', 52463), ('that', 50155), ('it', 49397), ('/><br', 38442), ('was', 35455), ('as', 34063), ('for', 32327), ('with', 32187), ('but', 30185), ('on', 23865), ('movie', 23178), ('his', 22218)]\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.freqs.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# defintion of data_loader\n",
    "mydevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(mydevice)\n",
    "train_iter, dev_iter, test_iter = data.BucketIterator.splits((train, dev, test), batch_size=16, device=mydevice, shuffle=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is specific batch to see the content of the train/test-iters\n",
    "batch = next(iter(train_iter))\n",
    "print(batch)\n",
    "print('batch.Text = \\n',batch.Text)\n",
    "print('batch.Label = \\n',batch.Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model architecture for sentiment classification: LSTM + MLP\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, \n",
    "                num_layers, bidirectional, drop_out, pad_idx, batch_first = False):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, \n",
    "                        padding_idx = pad_idx)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, \n",
    "                            batch_first = batch_first, bidirectional=bidirectional,\n",
    "                            dropout=drop_out)\n",
    "        '''\n",
    "        nn.LSTM(input_size, hidden_size, num_layers)\n",
    "        num_layers: the layer_num of LSTM, usually an important thing in LSTM-based model architecture...\n",
    "        bidirectional: also an important hyperparameter...\n",
    "        reference:https://blog.csdn.net/baidu_38963740/article/details/117197619?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link\n",
    "        '''\n",
    "        if bidirectional == False:\n",
    "            num_direction = 1\n",
    "        else:\n",
    "            num_direction = 2\n",
    "        lstm_output_dim = num_direction * hidden_dim\n",
    "\n",
    "        self.fc = nn.Linear(lstm_output_dim, 2)\n",
    "        # for this case is a 2-class problem\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        '''\n",
    "        x.shape = embedded.shape = (batch_size, seq_len, embedding_dim) [tips: when we set `batch_first` == True]\n",
    "        otherwise, x.shape = embedded.shape = (seq_len, bs, embedding_dim)\n",
    "        '''\n",
    "        lstm_output, (_, _) = self.lstm(embedded)\n",
    "        '''\n",
    "        when num_layers = bidirectional = 1 and batch_first = True\n",
    "        size of lstm_output: (batch_size, seq_len, hidden_dim * num_directions)\n",
    "        size of h_n and c_n: (num_layers * num_directions = 1, batch_size, hidden_size) \n",
    "        '''\n",
    "\n",
    "        output = self.dropout(self.fc(lstm_output[:, -1, :]))\n",
    "        '''\n",
    "        we only select last-step of seq_len in the lstm_output as \n",
    "        the encoding sentence vector, for it is containing the information\n",
    "        of the whole sentence(unidirectionally speaking),\n",
    "        when we adapt bidirectional lstm, we can choose any-step of seq_len\n",
    "        instead.\n",
    "        '''\n",
    "        '''\n",
    "        output:(batch_size, encoding_vector_dim=2)\n",
    "        '''\n",
    "        return F.log_softmax(output, dim = 1)\n",
    "\n",
    "pad_idx = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "# definition of model and optimizer\n",
    "model = LSTM(len(TEXT.vocab.stoi), 100, 128, 2, False, 0.4, pad_idx, True)\n",
    "model.embedding.weight.data = TEXT.vocab.vectors\n",
    "model.embedding.weight.requires_grad = False\n",
    "# frozen pretrained embedding weights\n",
    "\n",
    "model = model.cuda()\n",
    "'''\n",
    "(self, vocab_size, embedding_dim, hidden_dim, \n",
    "num_layers, bidirectional, drop_out, pad_idx, batch_first = False)\n",
    "'''\n",
    "opt = torch.optim.Adam(model.parameters(),lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training function\n",
    "def train_epoch(model, opt, data_loader, phase='training'):\n",
    "    '''\n",
    "    function: train model with opt for one epoch\n",
    "    '''\n",
    "    if phase == 'training':\n",
    "        model.train()\n",
    "    if (phase == 'validation') or (phase == 'testing'):\n",
    "        model.eval()\n",
    "    # model.train() : open `batch_normalization` and `drop_out`\n",
    "    # model.eval() : open `batch_normalization`, close `drop_out`\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0.0\n",
    "    for _, batch in enumerate(data_loader):\n",
    "        text, target = batch.Text, batch.Label\n",
    "        if mydevice == 'cuda':\n",
    "            text, target = text.cuda(),target.cuda()\n",
    "        if phase == 'training':\n",
    "            opt.zero_grad()\n",
    "        output = model(text)\n",
    "       \n",
    "        loss = F.nll_loss(output, target-1)\n",
    "        running_loss = F.nll_loss(output, target-1, size_average=False).data\n",
    "        preds = output.data.max(dim=1, keepdim=True)[1] + 1\n",
    "        # for label '0' -> 1(in vocab); label '1' -> 2(in vocab);\n",
    "        running_correct += preds.eq(target.data.view_as(preds)).sum()\n",
    "        if phase == 'training':\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "    \n",
    "    running_loss = running_loss.type(torch.FloatTensor)\n",
    "    running_correct = running_correct.type(torch.FloatTensor)\n",
    "    \n",
    "    # IMPORTANT above! otherwise accuracy will be zero all the time!\n",
    "    loss = running_loss/len(data_loader.dataset)\n",
    "    accuracy = running_correct/len(data_loader.dataset)\n",
    "    # print(type(loss),type(accuracy))\n",
    "    \n",
    " \n",
    "    print(f'{phase} loss is {loss:{5}.{2}} and {phase} accuracy is {running_correct}/{len(data_loader.dataset)} {accuracy:{10}.{4}}')\n",
    "    return loss,accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---the  1 's training starts---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/home/zhaoyi/anaconda3/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss is 0.00058 and training accuracy is 9958.0/18793     0.5299\n",
      "validation loss is 0.0018 and validation accuracy is 3159.0/6207     0.5089\n",
      "---the  1 's training ends---\n",
      "---the  2 's training starts---\n",
      "training loss is 0.00059 and training accuracy is 9513.0/18793     0.5062\n",
      "validation loss is 0.0018 and validation accuracy is 3064.0/6207     0.4936\n",
      "---the  2 's training ends---\n",
      "---the  3 's training starts---\n",
      "training loss is 0.00056 and training accuracy is 9808.0/18793     0.5219\n",
      "validation loss is 0.0017 and validation accuracy is 3584.0/6207     0.5774\n",
      "---the  3 's training ends---\n",
      "---the  4 's training starts---\n",
      "training loss is 0.00059 and training accuracy is 10131.0/18793     0.5391\n",
      "validation loss is 0.0018 and validation accuracy is 3252.0/6207     0.5239\n",
      "---the  4 's training ends---\n",
      "---the  5 's training starts---\n",
      "training loss is 0.00053 and training accuracy is 11412.0/18793     0.6072\n",
      "validation loss is 0.0016 and validation accuracy is 4349.0/6207     0.7007\n",
      "---the  5 's training ends---\n",
      "---the  6 's training starts---\n",
      "training loss is 0.00039 and training accuracy is 12636.0/18793     0.6724\n",
      "validation loss is 0.0015 and validation accuracy is 4468.0/6207     0.7198\n",
      "---the  6 's training ends---\n",
      "---the  7 's training starts---\n",
      "training loss is 0.00044 and training accuracy is 12919.0/18793     0.6874\n",
      "validation loss is 0.0017 and validation accuracy is 4496.0/6207     0.7243\n",
      "---the  7 's training ends---\n",
      "---the  8 's training starts---\n",
      "training loss is 0.00088 and training accuracy is 13171.0/18793     0.7008\n",
      "validation loss is 0.001 and validation accuracy is 4495.0/6207     0.7242\n",
      "---the  8 's training ends---\n",
      "---the  9 's training starts---\n",
      "training loss is 0.0007 and training accuracy is 13491.0/18793     0.7179\n",
      "validation loss is 0.0014 and validation accuracy is 4606.0/6207     0.7421\n",
      "---the  9 's training ends---\n",
      "---the  10 's training starts---\n",
      "training loss is 0.00038 and training accuracy is 13565.0/18793     0.7218\n",
      "validation loss is 0.002 and validation accuracy is 4600.0/6207     0.7411\n",
      "---the  10 's training ends---\n",
      "---the  11 's training starts---\n",
      "training loss is 0.00031 and training accuracy is 13863.0/18793     0.7377\n",
      "validation loss is 0.00092 and validation accuracy is 4636.0/6207     0.7469\n",
      "---the  11 's training ends---\n",
      "---the  12 's training starts---\n",
      "training loss is 0.00073 and training accuracy is 14075.0/18793     0.7489\n",
      "validation loss is 0.001 and validation accuracy is 4553.0/6207     0.7335\n",
      "---the  12 's training ends---\n",
      "---the  13 's training starts---\n",
      "training loss is 0.00049 and training accuracy is 14412.0/18793     0.7669\n",
      "validation loss is 0.0011 and validation accuracy is 4622.0/6207     0.7446\n",
      "---the  13 's training ends---\n",
      "---the  14 's training starts---\n",
      "training loss is 0.00031 and training accuracy is 14734.0/18793      0.784\n",
      "validation loss is 0.0014 and validation accuracy is 4602.0/6207     0.7414\n",
      "---the  14 's training ends---\n",
      "---the  15 's training starts---\n",
      "training loss is 0.00029 and training accuracy is 14873.0/18793     0.7914\n",
      "validation loss is 0.0023 and validation accuracy is 4591.0/6207     0.7396\n",
      "---the  15 's training ends---\n",
      "---the  16 's training starts---\n",
      "training loss is 0.00021 and training accuracy is 15197.0/18793     0.8087\n",
      "validation loss is 0.0019 and validation accuracy is 4591.0/6207     0.7396\n",
      "---the  16 's training ends---\n",
      "---the  17 's training starts---\n",
      "training loss is 0.00031 and training accuracy is 15466.0/18793      0.823\n",
      "validation loss is 0.0024 and validation accuracy is 4554.0/6207     0.7337\n",
      "---the  17 's training ends---\n",
      "---the  18 's training starts---\n",
      "training loss is 0.00018 and training accuracy is 15706.0/18793     0.8357\n",
      "validation loss is 0.00061 and validation accuracy is 4560.0/6207     0.7347\n",
      "---the  18 's training ends---\n",
      "---the  19 's training starts---\n",
      "training loss is 0.00026 and training accuracy is 15904.0/18793     0.8463\n",
      "validation loss is 0.00057 and validation accuracy is 4515.0/6207     0.7274\n",
      "---the  19 's training ends---\n",
      "---the  20 's training starts---\n",
      "training loss is 0.00014 and training accuracy is 16086.0/18793      0.856\n",
      "validation loss is 0.0024 and validation accuracy is 4465.0/6207     0.7193\n",
      "---the  20 's training ends---\n",
      "---testing phase---\n",
      "testing loss is 0.00027 and testing accuracy is 22361.0/25000     0.8944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.0003), tensor(0.8944))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect results\n",
    "train_losses, train_accuracy = [], []\n",
    "val_losses, val_accuracy = [], []\n",
    "\n",
    "train_iter.repeat = False\n",
    "test_iter.repeat = False\n",
    "\n",
    "epoch_max = 20\n",
    "for epoch in range(1,epoch_max+1):\n",
    "    print('---the ',epoch,\"'s training starts---\")\n",
    "    epoch_loss, epoch_accuracy = train_epoch(model, opt, train_iter, phase='training')\n",
    "    val_epoch_loss, val_epoch_accuracy = train_epoch(model, opt, dev_iter, phase='validation')\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracy.append(epoch_accuracy)\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    val_accuracy.append(val_epoch_accuracy)\n",
    "    print('---the ',epoch,\"'s training ends---\")\n",
    "\n",
    "print('---testing phase---')\n",
    "# test model's performance\n",
    "train_epoch(model, opt, test_iter, phase='testing')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "19e1942432a84160b99a17f87e7e4600300f515ff4805ea7b2f6199b7a48d87c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
