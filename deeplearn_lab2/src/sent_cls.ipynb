{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "os.chdir('/data2/home/zhaoyi/labs/USTC-labs/deeplearn_lab2')\n",
    "import torch\n",
    "import torchtext\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchtext.legacy import data, datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(lower=True, batch_first=True, fix_length=50)\n",
    "LABEL = data.Field(sequential=False)\n",
    "train, dev, test = data.TabularDataset.splits(path='/data2/home/zhaoyi/labs/USTC-labs/deeplearn_lab2/dataset/procd/', train='train.csv', validation='dev.csv', test='test.csv', \n",
    "                format='csv', fields=[('Text',TEXT),('Label',LABEL)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://zhuanlan.zhihu.com/p/447309785  word2vec approaches\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/562237953 pretrained word2vecs (e.g. glove of different versions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct and load word-vectors from a pretrained file\n",
    "TEXT.build_vocab(train, vectors=\"glove.6B.100d\", max_size=10000, min_freq=10)\n",
    "# glove-file-location : workspace/.vector_cache\n",
    "LABEL.build_vocab(train)\n",
    "# print(TEXT.vocab.freqs.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TEXT.vocab.freqs.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defintion of data_loader\n",
    "mydevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(mydevice)\n",
    "train_iter, dev_iter, test_iter = data.BucketIterator.splits((train, dev, test), batch_size=16, device=mydevice, shuffle=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is specific batch to see the content of the train/test-iters\n",
    "batch = next(iter(train_iter))\n",
    "print(batch)\n",
    "print('batch.Text = \\n',batch.Text)\n",
    "print('batch.Label = \\n',batch.Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model architecture for sentiment classification: LSTM + MLP\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, \n",
    "                num_layers, bidirectional, drop_out, pad_idx, batch_first = False):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, \n",
    "                        padding_idx = pad_idx)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, \n",
    "                            batch_first = batch_first, bidirectional=bidirectional,\n",
    "                            dropout=drop_out)\n",
    "        '''\n",
    "        nn.LSTM(input_size, hidden_size, num_layers)\n",
    "        num_layers: the layer_num of LSTM, usually an important thing in LSTM-based model architecture...\n",
    "        bidirectional: also an important hyperparameter...\n",
    "        reference:https://blog.csdn.net/baidu_38963740/article/details/117197619?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link\n",
    "        '''\n",
    "        if bidirectional == False:\n",
    "            num_direction = 1\n",
    "        else:\n",
    "            num_direction = 2\n",
    "        lstm_output_dim = num_direction * hidden_dim\n",
    "\n",
    "        self.fc = nn.Linear(lstm_output_dim, 2)\n",
    "        # for this case is a 2-class problem\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        '''\n",
    "        x.shape = embedded.shape = (batch_size, seq_len, embedding_dim) [tips: when we set `batch_first` == True]\n",
    "        otherwise, x.shape = embedded.shape = (seq_len, bs, embedding_dim)\n",
    "        '''\n",
    "        lstm_output, (_, _) = self.lstm(embedded)\n",
    "        '''\n",
    "        when num_layers = bidirectional = 1 and batch_first = True\n",
    "        size of lstm_output: (batch_size, seq_len, hidden_dim * num_directions)\n",
    "        size of h_n and c_n: (num_layers * num_directions = 1, batch_size, hidden_size) \n",
    "        '''\n",
    "\n",
    "        output = self.dropout(self.fc(lstm_output[:, -1, :]))\n",
    "        '''\n",
    "        we only select last-step of seq_len in the lstm_output as \n",
    "        the encoding sentence vector, for it is containing the information\n",
    "        of the whole sentence(unidirectionally speaking),\n",
    "        when we adapt bidirectional lstm, we can choose any-step of seq_len\n",
    "        instead.\n",
    "        '''\n",
    "        '''\n",
    "        output:(batch_size, encoding_vector_dim=2)\n",
    "        '''\n",
    "        return F.log_softmax(output, dim = 1)\n",
    "\n",
    "pad_idx = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "# definition of model and optimizer\n",
    "model = LSTM(len(TEXT.vocab.stoi), 100, 128, 2, False, 0.4, pad_idx, True)\n",
    "model.embedding.weight.data = TEXT.vocab.vectors\n",
    "model.embedding.weight.requires_grad = False\n",
    "# frozen pretrained embedding weights\n",
    "\n",
    "model = model.cuda()\n",
    "'''\n",
    "(self, vocab_size, embedding_dim, hidden_dim, \n",
    "num_layers, bidirectional, drop_out, pad_idx, batch_first = False)\n",
    "'''\n",
    "opt = torch.optim.Adam(model.parameters(),lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training function\n",
    "def train_epoch(model, opt, data_loader, phase='training'):\n",
    "    '''\n",
    "    function: train model with opt for one epoch\n",
    "    '''\n",
    "    if phase == 'training':\n",
    "        model.train()\n",
    "    if (phase == 'validation') or (phase == 'testing'):\n",
    "        model.eval()\n",
    "    # model.train() : open `batch_normalization` and `drop_out`\n",
    "    # model.eval() : open `batch_normalization`, close `drop_out`\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0.0\n",
    "    for _, batch in enumerate(data_loader):\n",
    "        text, target = batch.Text, batch.Label\n",
    "        if mydevice == 'cuda':\n",
    "            text, target = text.cuda(),target.cuda()\n",
    "        if phase == 'training':\n",
    "            opt.zero_grad()\n",
    "        output = model(text)\n",
    "       \n",
    "        loss = F.nll_loss(output, target-1)\n",
    "        running_loss = F.nll_loss(output, target-1, size_average=False).data\n",
    "        preds = output.data.max(dim=1, keepdim=True)[1] + 1\n",
    "        # for label '0' -> 1(in vocab); label '1' -> 2(in vocab);\n",
    "        running_correct += preds.eq(target.data.view_as(preds)).sum()\n",
    "        if phase == 'training':\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "    \n",
    "    running_loss = running_loss.type(torch.FloatTensor)\n",
    "    running_correct = running_correct.type(torch.FloatTensor)\n",
    "    \n",
    "    # IMPORTANT above! otherwise accuracy will be zero all the time!\n",
    "    loss = running_loss/len(data_loader.dataset)\n",
    "    accuracy = running_correct/len(data_loader.dataset)\n",
    "    # print(type(loss),type(accuracy))\n",
    "    \n",
    " \n",
    "    print(f'{phase} loss is {loss:{5}.{2}} and {phase} accuracy is {running_correct}/{len(data_loader.dataset)} {accuracy:{10}.{4}}')\n",
    "    return loss,accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect results\n",
    "train_losses, train_accuracy = [], []\n",
    "val_losses, val_accuracy = [], []\n",
    "\n",
    "train_iter.repeat = False\n",
    "test_iter.repeat = False\n",
    "\n",
    "epoch_max = 20\n",
    "for epoch in range(1,epoch_max+1):\n",
    "    print('---the ',epoch,\"'s training starts---\")\n",
    "    epoch_loss, epoch_accuracy = train_epoch(model, opt, train_iter, phase='training')\n",
    "    val_epoch_loss, val_epoch_accuracy = train_epoch(model, opt, dev_iter, phase='validation')\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracy.append(epoch_accuracy)\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    val_accuracy.append(val_epoch_accuracy)\n",
    "    print('---the ',epoch,\"'s training ends---\")\n",
    "\n",
    "# test model's performance\n",
    "train_epoch(model, opt, test_iter, phase='testing')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "19e1942432a84160b99a17f87e7e4600300f515ff4805ea7b2f6199b7a48d87c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
