0.002513776878247484
(tensor([6.5132], device='cuda:0'), tensor([0.2295], device='cuda:0'))
3500
Net(
  (inp_layer): Linear(in_features=1, out_features=20, bias=True)
  (hiddens): ModuleList(
    (0): Linear(in_features=20, out_features=20, bias=True)
  )
  (out_layer): Linear(in_features=20, out_features=1, bias=True)
)
step: 10 , loss: 0.3941023647785187
0.31764402985572815
step: 20 , loss: 0.25287723541259766
0.26496294140815735
step: 30 , loss: 0.18778394162654877
0.19097226858139038
step: 40 , loss: 0.10282959043979645
0.12743636965751648
step: 50 , loss: 0.1034746989607811
0.09176959842443466
step: 60 , loss: 0.07773192226886749
0.07888507097959518
step: 70 , loss: 0.03701375052332878
0.07919148355722427
step: 80 , loss: 0.071125328540802
0.07558932155370712
step: 90 , loss: 0.041953954845666885
0.07486827671527863
step: 100 , loss: 0.08016856759786606
0.07485135644674301
step: 110 , loss: 0.06719314306974411
0.07509690523147583
step: 120 , loss: 0.08088325709104538
0.07602614909410477
step: 130 , loss: 0.047985393553972244
0.075975202023983
step: 140 , loss: 0.06958582252264023
0.07847516983747482
step: 150 , loss: 0.06517525017261505
0.07572506368160248
step: 160 , loss: 0.04841871187090874
0.07631206512451172
step: 170 , loss: 0.03395320102572441
0.07510662823915482
step: 180 , loss: 0.07898537814617157
0.07371704280376434
step: 190 , loss: 0.040120333433151245
0.06772109121084213
step: 200 , loss: 0.028611520305275917
0.06325821578502655
step: 210 , loss: 0.07037245482206345
0.05591599643230438
step: 220 , loss: 0.04578180983662605
0.049740955233573914
step: 230 , loss: 0.025951728224754333
0.04532743617892265
step: 240 , loss: 0.04233582690358162
0.04035428538918495
step: 250 , loss: 0.03357704356312752
0.03521107882261276
step: 260 , loss: 0.019927849993109703
0.031320247799158096
step: 270 , loss: 0.01871730573475361
0.028648657724261284
step: 280 , loss: 0.033801186829805374
0.025828715413808823
step: 290 , loss: 0.020855532959103584
0.022352861240506172
step: 300 , loss: 0.021373210474848747
0.0195942185819149
step: 310 , loss: 0.032074376940727234
0.01715340092778206
step: 320 , loss: 0.027312545105814934
0.015244689770042896
step: 330 , loss: 0.009426025673747063
0.013432583771646023
step: 340 , loss: 0.012457449920475483
0.01160792913287878
step: 350 , loss: 0.011591784656047821
0.011051613837480545
step: 360 , loss: 0.013343475759029388
0.009668029844760895
step: 370 , loss: 0.009690581820905209
0.008674416691064835
step: 380 , loss: 0.007387187331914902
0.007330321706831455
step: 390 , loss: 0.0069451043382287025
0.006344099063426256
step: 400 , loss: 0.004185185767710209
0.005802165251225233
step: 410 , loss: 0.0038334219716489315
0.005242503248155117
step: 420 , loss: 0.003964229486882687
0.004693313036113977
step: 430 , loss: 0.007771732751280069
0.004004995804280043
step: 440 , loss: 0.002029665047302842
0.0033665585797280073
step: 450 , loss: 0.0029192555230110884
0.003010472049936652
step: 460 , loss: 0.002371446928009391
0.0029980556573718786
step: 470 , loss: 0.002405395032837987
0.0024907661136239767
step: 480 , loss: 0.001882936805486679
0.0023242924362421036
step: 490 , loss: 0.001809983979910612
0.0023054403718560934
step: 500 , loss: 0.0016224972205236554
0.0021099159494042397
lr: 0.0005
activation function type: relu
depth: 2
width: 20
test_loss: 0.0024955319240689278
