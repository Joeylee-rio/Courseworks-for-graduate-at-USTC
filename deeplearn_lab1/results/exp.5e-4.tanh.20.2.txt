0.002513776878247484
(tensor([6.5132], device='cuda:0'), tensor([0.2295], device='cuda:0'))
3500
Net(
  (inp_layer): Linear(in_features=1, out_features=20, bias=True)
  (hiddens): ModuleList(
    (0): Linear(in_features=20, out_features=20, bias=True)
  )
  (out_layer): Linear(in_features=20, out_features=1, bias=True)
)
step: 10 , loss: 0.44861236214637756
0.36001691222190857
step: 20 , loss: 0.3394324779510498
0.34766972064971924
step: 30 , loss: 0.3150334656238556
0.33224162459373474
step: 40 , loss: 0.28979942202568054
0.29104509949684143
step: 50 , loss: 0.17775733768939972
0.21450306475162506
step: 60 , loss: 0.13733124732971191
0.1540193110704422
step: 70 , loss: 0.0799693763256073
0.13892652094364166
step: 80 , loss: 0.11712740361690521
0.1144411638379097
step: 90 , loss: 0.07012823969125748
0.1065049022436142
step: 100 , loss: 0.10303255170583725
0.1000882163643837
step: 110 , loss: 0.07743560522794724
0.0924587994813919
step: 120 , loss: 0.08874303102493286
0.08675508201122284
step: 130 , loss: 0.05396796390414238
0.08227728307247162
step: 140 , loss: 0.06950344145298004
0.08143935352563858
step: 150 , loss: 0.06245490163564682
0.07036031782627106
step: 160 , loss: 0.0442536398768425
0.06604550033807755
step: 170 , loss: 0.02772427536547184
0.06320641189813614
step: 180 , loss: 0.06360334157943726
0.06012674793601036
step: 190 , loss: 0.035401601344347
0.058498721569776535
step: 200 , loss: 0.02776814065873623
0.05673229321837425
step: 210 , loss: 0.06810928881168365
0.05467842519283295
step: 220 , loss: 0.048683833330869675
0.05585356429219246
step: 230 , loss: 0.030777957290410995
0.05266834422945976
step: 240 , loss: 0.0574030876159668
0.05158410221338272
step: 250 , loss: 0.0461796410381794
0.05222085863351822
step: 260 , loss: 0.03252636268734932
0.05210677906870842
step: 270 , loss: 0.03376767039299011
0.050411492586135864
step: 280 , loss: 0.06054649129509926
0.049642257392406464
step: 290 , loss: 0.053617652505636215
0.0509616881608963
step: 300 , loss: 0.04379107058048248
0.0490238219499588
step: 310 , loss: 0.08624177426099777
0.048456259071826935
step: 320 , loss: 0.0827309712767601
0.04729780927300453
step: 330 , loss: 0.031169941648840904
0.04672088101506233
step: 340 , loss: 0.044556546956300735
0.04646102711558342
step: 350 , loss: 0.05165477469563484
0.05269373953342438
step: 360 , loss: 0.05948648974299431
0.04593124985694885
step: 370 , loss: 0.06517219543457031
0.045600324869155884
step: 380 , loss: 0.0505891889333725
0.045403141528367996
step: 390 , loss: 0.052700143307447433
0.04480670765042305
step: 400 , loss: 0.03250814229249954
0.04518396034836769
step: 410 , loss: 0.024431949481368065
0.046058930456638336
step: 420 , loss: 0.026065101847052574
0.044331490993499756
step: 430 , loss: 0.08215192705392838
0.04409768804907799
step: 440 , loss: 0.02294127643108368
0.04415520653128624
step: 450 , loss: 0.023969558998942375
0.0427815243601799
step: 460 , loss: 0.020170342177152634
0.04529346525669098
step: 470 , loss: 0.02663721889257431
0.042056381702423096
step: 480 , loss: 0.0281758364289999
0.04255395010113716
step: 490 , loss: 0.025470798835158348
0.041829030960798264
step: 500 , loss: 0.008256793022155762
0.04142266884446144
lr: 0.0005
activation function type: tanh
depth: 2
width: 20
test_loss: 0.03909781575202942
