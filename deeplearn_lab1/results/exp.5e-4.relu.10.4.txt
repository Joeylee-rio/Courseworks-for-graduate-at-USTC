Net(
  (inp_layer): Linear(in_features=1, out_features=10, bias=True)
  (hiddens): ModuleList(
    (0): Linear(in_features=10, out_features=10, bias=True)
    (1): Linear(in_features=10, out_features=10, bias=True)
    (2): Linear(in_features=10, out_features=10, bias=True)
  )
  (out_layer): Linear(in_features=10, out_features=1, bias=True)
)
step: 10 , loss: 0.3552370071411133
0.37678632140159607
step: 20 , loss: 0.20230282843112946
0.3036348819732666
step: 30 , loss: 0.14165939390659332
0.11337155848741531
step: 40 , loss: 0.01813807711005211
0.01958542875945568
step: 50 , loss: 0.021284354850649834
0.01593877375125885
step: 60 , loss: 0.0189015232026577
0.014568395912647247
step: 70 , loss: 0.01864089071750641
0.015158944763243198
step: 80 , loss: 0.0036198608577251434
0.012375520542263985
step: 90 , loss: 0.01737661473453045
0.011286158114671707
step: 100 , loss: 0.004880405496805906
0.010072045959532261
step: 110 , loss: 0.004619990941137075
0.009222940541803837
step: 120 , loss: 0.007957984693348408
0.007288886234164238
step: 130 , loss: 0.008548617362976074
0.006113624665886164
step: 140 , loss: 0.005768530070781708
0.0053298803977668285
step: 150 , loss: 0.003431288991123438
0.004280955530703068
step: 160 , loss: 0.007827500812709332
0.004497438203543425
step: 170 , loss: 0.006212207023054361
0.0031508051324635744
step: 180 , loss: 0.001522572711110115
0.0018769081216305494
step: 190 , loss: 0.0020685619674623013
0.0015889564529061317
step: 200 , loss: 0.0006377969984896481
0.00132282474078238
step: 210 , loss: 0.0011958717368543148
0.0011668575461953878
step: 220 , loss: 0.0005980664864182472
0.0014943521237000823
step: 230 , loss: 0.0006316702347248793
0.0011781631037592888
step: 240 , loss: 0.0007218598620966077
0.0013041167985647917
step: 250 , loss: 0.0005699257599189878
0.001158476690761745
step: 260 , loss: 0.0008435655618086457
0.0010941963410004973
step: 270 , loss: 0.00034123670775443316
0.0009878292912617326
step: 280 , loss: 0.0006314914207905531
0.0009119550813920796
step: 290 , loss: 0.0011461152462288737
0.0009881590958684683
step: 300 , loss: 0.0032074986957013607
0.001022327458485961
step: 310 , loss: 0.000644381798338145
0.0008790327701717615
step: 320 , loss: 0.0007286150939762592
0.0011181170120835304
step: 330 , loss: 0.00045220786705613136
0.0008967292960733175
step: 340 , loss: 0.000704810256138444
0.0007959660142660141
step: 350 , loss: 0.0005856417119503021
0.0009298012009821832
step: 360 , loss: 0.0006895179394632578
0.0007984753465279937
step: 370 , loss: 0.001622784067876637
0.0008220651652663946
step: 380 , loss: 0.0008370241848751903
0.0008701892220415175
step: 390 , loss: 0.0006964004132896662
0.0007912195287644863
step: 400 , loss: 0.0012334896018728614
0.0007747866911813617
step: 410 , loss: 0.0010738209821283817
0.0010763296158984303
step: 420 , loss: 0.0007072176085785031
0.0007394214626401663
step: 430 , loss: 0.0006130420370027423
0.0008948096656240523
step: 440 , loss: 0.0009878830751404166
0.0009261064697057009
step: 450 , loss: 0.0013814882840961218
0.0007764167967252433
step: 460 , loss: 0.001208268804475665
0.0007314396207220852
step: 470 , loss: 0.0007872516289353371
0.000727898208424449
step: 480 , loss: 0.0009355515940114856
0.0006811151979491115
step: 490 , loss: 0.0005861544632352889
0.000760802417062223
step: 500 , loss: 0.0004679860721807927
0.000645213935058564
lr: 0.0005
activation function type: relu
depth: 4
width: 10
test_loss: 0.0006478788563981652
