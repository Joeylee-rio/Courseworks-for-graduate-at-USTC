Net(
  (inp_layer): Linear(in_features=1, out_features=13, bias=True)
  (hiddens): ModuleList(
    (0): Linear(in_features=13, out_features=13, bias=True)
    (1): Linear(in_features=13, out_features=13, bias=True)
  )
  (out_layer): Linear(in_features=13, out_features=1, bias=True)
)
step: 10 , loss: 0.3051995038986206
0.28620949387550354
step: 20 , loss: 0.16896659135818481
0.12303261458873749
step: 30 , loss: 0.02124340459704399
0.03136291727423668
step: 40 , loss: 0.023040827363729477
0.024612292647361755
step: 50 , loss: 0.034146860241889954
0.023147499188780785
step: 60 , loss: 0.023432618007063866
0.0220761951059103
step: 70 , loss: 0.01606450043618679
0.02076120302081108
step: 80 , loss: 0.014727069064974785
0.015909317880868912
step: 90 , loss: 0.007140501867979765
0.011183756403625011
step: 100 , loss: 0.002797874854877591
0.00727033568546176
step: 110 , loss: 0.003987924661487341
0.0042355661280453205
step: 120 , loss: 0.003059819806367159
0.003173844190314412
step: 130 , loss: 0.002471787156537175
0.0024964530020952225
step: 140 , loss: 0.0026699842419475317
0.0020144593436270952
step: 150 , loss: 0.0023126844316720963
0.0020349551923573017
step: 160 , loss: 0.001418889150954783
0.0018726594280451536
step: 170 , loss: 0.0032338490709662437
0.0016734080854803324
step: 180 , loss: 0.000728074403014034
0.0020517536904662848
step: 190 , loss: 0.0015394098591059446
0.0016272193752229214
step: 200 , loss: 0.0012773610651493073
0.0017265831120312214
step: 210 , loss: 0.002017576713114977
0.001686005387455225
step: 220 , loss: 0.0009776765946298838
0.0015940087614580989
step: 230 , loss: 0.001199126010760665
0.0021644835360348225
step: 240 , loss: 0.0020898832008242607
0.001502105500549078
step: 250 , loss: 0.0009728391305543482
0.001522247213870287
step: 260 , loss: 0.0013258190592750907
0.0015121214091777802
step: 270 , loss: 0.0010468213586136699
0.001761742983944714
step: 280 , loss: 0.0014191529480740428
0.0017341832863166928
step: 290 , loss: 0.0011429889127612114
0.0014455456985160708
step: 300 , loss: 0.002396787516772747
0.0015797633677721024
step: 310 , loss: 0.001711202901788056
0.0014826653059571981
step: 320 , loss: 0.0011207022471353412
0.0015167670790106058
step: 330 , loss: 0.0008031752076931298
0.0014433804899454117
step: 340 , loss: 0.002262540627270937
0.0014450030867010355
step: 350 , loss: 0.0015357144875451922
0.0013927164254710078
step: 360 , loss: 0.0006967977969907224
0.001369662582874298
step: 370 , loss: 0.000984021695330739
0.0016323388554155827
step: 380 , loss: 0.0016917252214625478
0.0014462867984548211
step: 390 , loss: 0.0016739445272833109
0.0013644335558637977
step: 400 , loss: 0.0007364014163613319
0.0014413335593417287
step: 410 , loss: 0.0009409799240529537
0.0014144465094432235
step: 420 , loss: 0.0018072091042995453
0.0016824982594698668
step: 430 , loss: 0.001512482762336731
0.0015186286764219403
step: 440 , loss: 0.0007610580651089549
0.0013050433481112123
step: 450 , loss: 0.0006877498817630112
0.0014341776259243488
step: 460 , loss: 0.0007246847962960601
0.0015281615778803825
step: 470 , loss: 0.0009629913256503642
0.001262010307982564
step: 480 , loss: 0.0015986210200935602
0.0013070256682112813
step: 490 , loss: 0.0021437190007418394
0.0013614519266411662
step: 500 , loss: 0.0015088021755218506
0.0016658956883475184
lr: 0.0005
activation function type: relu
depth: 3
width: 13
test_loss: 0.0016936303582042456
