step: 10 , loss: 0.41938456892967224
0.3862091302871704
step: 20 , loss: 0.4570569097995758
0.34711530804634094
step: 30 , loss: 0.33309584856033325
0.3290152847766876
step: 40 , loss: 0.34192195534706116
0.3140428066253662
step: 50 , loss: 0.25670403242111206
0.2972671091556549
step: 60 , loss: 0.17702335119247437
0.2788139581680298
step: 70 , loss: 0.29571396112442017
0.2595180869102478
step: 80 , loss: 0.3095654845237732
0.23728324472904205
step: 90 , loss: 0.27172309160232544
0.21456444263458252
step: 100 , loss: 0.11509554088115692
0.19215388596057892
step: 110 , loss: 0.17832434177398682
0.17037102580070496
step: 120 , loss: 0.11928703635931015
0.15075945854187012
step: 130 , loss: 0.13975796103477478
0.13296858966350555
step: 140 , loss: 0.13090206682682037
0.11611784994602203
step: 150 , loss: 0.09951873123645782
0.1029721274971962
step: 160 , loss: 0.09346136450767517
0.09574928879737854
step: 170 , loss: 0.07640507072210312
0.08593986928462982
step: 180 , loss: 0.0609765462577343
0.0815061703324318
step: 190 , loss: 0.08212059736251831
0.07769086211919785
step: 200 , loss: 0.032618362456560135
0.07609132677316666
step: 210 , loss: 0.03521031141281128
0.07366418093442917
step: 220 , loss: 0.07075844705104828
0.07261506468057632
step: 230 , loss: 0.04553135856986046
0.07358013093471527
step: 240 , loss: 0.0686749592423439
0.0714576467871666
step: 250 , loss: 0.029148051515221596
0.07185836881399155
step: 260 , loss: 0.04325268417596817
0.0709303691983223
step: 270 , loss: 0.12444978952407837
0.07147920876741409
step: 280 , loss: 0.04457564651966095
0.07188251614570618
step: 290 , loss: 0.12739770114421844
0.07058020681142807
step: 300 , loss: 0.09534798562526703
0.07068778574466705
step: 310 , loss: 0.042986705899238586
0.07149199396371841
step: 320 , loss: 0.048842959105968475
0.07120398432016373
step: 330 , loss: 0.08862078189849854
0.0706290677189827
step: 340 , loss: 0.11548538506031036
0.07042673230171204
step: 350 , loss: 0.027665864676237106
0.07080363482236862
step: 360 , loss: 0.04601135849952698
0.07038252800703049
step: 370 , loss: 0.07843632996082306
0.07037213444709778
step: 380 , loss: 0.0635603815317154
0.0703417956829071
step: 390 , loss: 0.048358555883169174
0.07042985409498215
step: 400 , loss: 0.026856979355216026
0.07141461223363876
step: 410 , loss: 0.06089776009321213
0.07120303809642792
step: 420 , loss: 0.08109044283628464
0.07112191617488861
step: 430 , loss: 0.13601171970367432
0.0707780122756958
step: 440 , loss: 0.03329915553331375
0.0703529417514801
step: 450 , loss: 0.04050541669130325
0.07056452333927155
step: 460 , loss: 0.05593159794807434
0.07179374992847443
step: 470 , loss: 0.0785108283162117
0.07045049965381622
step: 480 , loss: 0.0760919600725174
0.07039527595043182
step: 490 , loss: 0.07012908160686493
0.07055803388357162
step: 500 , loss: 0.1561877727508545
0.07059592753648758
lr: 0.001
activation function type: relu
depth: 3
width: 13
test_loss: 0.07432770729064941
