0.003142378248151831
Net(
  (inp_layer): Linear(in_features=1, out_features=20, bias=True)
  (hiddens): ModuleList(
    (0): Linear(in_features=20, out_features=20, bias=True)
  )
  (out_layer): Linear(in_features=20, out_features=1, bias=True)
)
0.025183107443605555
step: 10 , loss: 0.23692062497138977
0.2131587117910385
step: 20 , loss: 0.06465166807174683
0.05124644562602043
step: 30 , loss: 0.04566849023103714
0.03962598741054535
step: 40 , loss: 0.03663503751158714
0.02876739762723446
step: 50 , loss: 0.010599525645375252
0.02537355199456215
step: 60 , loss: 0.028560282662510872
0.02021081931889057
step: 70 , loss: 0.009571123868227005
0.018650010228157043
step: 80 , loss: 0.00447489321231842
0.013143503107130527
step: 90 , loss: 0.012594405561685562
0.01061344612389803
step: 100 , loss: 0.01085575483739376
0.008208617568016052
step: 110 , loss: 0.008056029677391052
0.0070941755548119545
step: 120 , loss: 0.0040137264877557755
0.004970130976289511
step: 130 , loss: 0.007190327160060406
0.004889749921858311
step: 140 , loss: 0.006397594232112169
0.002914250362664461
step: 150 , loss: 0.0018429365009069443
0.0030496413819491863
step: 160 , loss: 0.003196986624971032
0.0025982914958149195
step: 170 , loss: 0.0005996639374643564
0.002460523508489132
step: 180 , loss: 0.0015377202071249485
0.0013038883917033672
step: 190 , loss: 0.000978624913841486
0.0014453179901465774
step: 200 , loss: 0.001516187097877264
0.0014682739274576306
step: 210 , loss: 0.0002718669711612165
0.0008771927678026259
step: 220 , loss: 0.0007716763066127896
0.0008289086981676519
step: 230 , loss: 0.0009127309895120561
0.0010159285739064217
step: 240 , loss: 0.0005748053081333637
0.0024786668363958597
step: 250 , loss: 0.0005941085400991142
0.0006541291368193924
step: 260 , loss: 0.000988472136668861
0.000586712034419179
step: 270 , loss: 0.001135650323703885
0.0005394307663664222
step: 280 , loss: 0.0011943813879042864
0.0014529769541695714
step: 290 , loss: 0.0004756017297040671
0.0005685269134119153
step: 300 , loss: 0.0006151818670332432
0.0004930742434225976
step: 310 , loss: 0.0007261828286573291
0.0004949760041199625
step: 320 , loss: 0.00034186866832897067
0.0004755099362228066
step: 330 , loss: 0.0003234354080632329
0.0007716540130786598
step: 340 , loss: 0.0007997258799150586
0.0005982399452477694
step: 350 , loss: 0.0006372373318299651
0.0005715283332392573
step: 360 , loss: 0.0010653638746589422
0.0007915587048046291
step: 370 , loss: 0.00041938055073842406
0.0004963584360666573
step: 380 , loss: 0.0005488571478053927
0.0004384244093671441
step: 390 , loss: 0.0015607555396854877
0.0008137092809192836
step: 400 , loss: 0.0005713966093026102
0.0004473830049391836
step: 410 , loss: 0.00046565980301238596
0.0009645367390476167
step: 420 , loss: 0.0008148479973897338
0.0006918137660250068
step: 430 , loss: 0.0004445532395038754
0.0003967616939917207
step: 440 , loss: 0.0019713810179382563
0.0006543055060319602
step: 450 , loss: 0.0006630239658989012
0.0005444189300760627
step: 460 , loss: 0.0003360932460054755
0.0006381695857271552
step: 470 , loss: 0.001424920978024602
0.0010918219340965152
step: 480 , loss: 0.00032792685669846833
0.00042426903382875025
step: 490 , loss: 0.00032512692268937826
0.0017100500408560038
step: 500 , loss: 0.0008440505480393767
0.0008844154071994126
0.09894780011306435
lr: 0.001
activation function type: relu
depth: 2
width: 20
test_loss: 0.000932958209887147
