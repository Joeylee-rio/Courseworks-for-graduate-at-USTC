Net(
  (inp_layer): Linear(in_features=1, out_features=20, bias=True)
  (hiddens): ModuleList(
    (0): Linear(in_features=20, out_features=20, bias=True)
  )
  (out_layer): Linear(in_features=20, out_features=1, bias=True)
)
step: 10 , loss: 0.030107595026493073
0.0411791168153286
step: 20 , loss: 0.024121694266796112
0.016335822641849518
step: 30 , loss: 0.011621181853115559
0.00880352407693863
step: 40 , loss: 0.010867157950997353
0.010910216718912125
step: 50 , loss: 0.008760558441281319
0.009430991485714912
step: 60 , loss: 0.004711520858108997
0.0023413512390106916
step: 70 , loss: 0.0015865638852119446
0.004435948561877012
step: 80 , loss: 0.002628730610013008
0.002020830288529396
step: 90 , loss: 0.001180695602670312
0.0014279725728556514
step: 100 , loss: 0.001122885849326849
0.0007527290727011859
step: 110 , loss: 0.0010334275430068374
0.0009212045697495341
step: 120 , loss: 0.0009397236863151193
0.001476024859584868
step: 130 , loss: 0.0010902509093284607
0.0007472756551578641
step: 140 , loss: 0.0011145746102556586
0.0009621352073736489
step: 150 , loss: 0.002218118868768215
0.0029448901768773794
step: 160 , loss: 0.048230964690446854
0.004786534700542688
step: 170 , loss: 0.0004763099132105708
0.0017542721470817924
step: 180 , loss: 0.0010579293593764305
0.0015957198338583112
step: 190 , loss: 0.0004975777701474726
0.0029545812867581844
step: 200 , loss: 0.0008044705609790981
0.0010686063906177878
step: 210 , loss: 0.0004749970103148371
0.0004401177866384387
step: 220 , loss: 0.002118706237524748
0.004352792166173458
step: 230 , loss: 0.007913170382380486
0.0020882815588265657
step: 240 , loss: 0.0010191788896918297
0.0004961512750014663
step: 250 , loss: 0.0033746182452887297
0.001853185473009944
step: 260 , loss: 0.0006025861366651952
0.0007117369677871466
step: 270 , loss: 0.0007472572033293545
0.0006138308090157807
step: 280 , loss: 0.0003592414141166955
0.0004805789503734559
step: 290 , loss: 0.00021620385814458132
0.0004140697419643402
step: 300 , loss: 0.000555961043573916
0.005160864908248186
step: 310 , loss: 0.0005870580207556486
0.00042237062007188797
step: 320 , loss: 0.002302641747519374
0.00045649369712918997
step: 330 , loss: 0.0012170338304713368
0.0005812607705593109
step: 340 , loss: 0.0022710254415869713
0.0008395408513024449
step: 350 , loss: 0.000616084726061672
0.0005306380335241556
step: 360 , loss: 0.0008587540360167623
0.004006280098110437
step: 370 , loss: 0.00039821257814764977
0.00040216417983174324
step: 380 , loss: 0.0003922938194591552
0.0004158595984335989
step: 390 , loss: 0.009674180299043655
0.0158269964158535
step: 400 , loss: 0.00030169085948728025
0.0003823525330517441
step: 410 , loss: 0.0005026452126912773
0.000502769136801362
step: 420 , loss: 0.00021851452765986323
0.0005391982849687338
step: 430 , loss: 0.0007692197686992586
0.0004293205856811255
step: 440 , loss: 0.0004911381984129548
0.00031739569385536015
step: 450 , loss: 0.00046208815183490515
0.0006016870029270649
step: 460 , loss: 0.00038095322088338435
0.00038773793494328856
step: 470 , loss: 0.0008469362510368228
0.0004403240163810551
step: 480 , loss: 0.0008351932046934962
0.00035983958514407277
step: 490 , loss: 0.0009323629783466458
0.0017450298182666302
step: 500 , loss: 0.00035750571987591684
0.0013149785809218884
lr: 0.005
activation function type: relu
depth: 2
width: 20
test_loss: 0.0013534226454794407
