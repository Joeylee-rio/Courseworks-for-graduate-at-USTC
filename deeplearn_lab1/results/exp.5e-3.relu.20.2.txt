0.002513776878247484
(tensor([6.5132], device='cuda:0'), tensor([0.2295], device='cuda:0'))
3500
Net(
  (inp_layer): Linear(in_features=1, out_features=20, bias=True)
  (hiddens): ModuleList(
    (0): Linear(in_features=20, out_features=20, bias=True)
  )
  (out_layer): Linear(in_features=20, out_features=1, bias=True)
)
step: 10 , loss: 0.3874645233154297
0.32421067357063293
step: 20 , loss: 0.196046382188797
0.21022358536720276
step: 30 , loss: 0.026978110894560814
0.0383358970284462
step: 40 , loss: 0.026590649038553238
0.027154210954904556
step: 50 , loss: 0.01908598653972149
0.009897992014884949
step: 60 , loss: 0.005947563797235489
0.006193138193339109
step: 70 , loss: 0.0036828613374382257
0.0051696086302399635
step: 80 , loss: 0.011781156063079834
0.007581339217722416
step: 90 , loss: 0.006920192390680313
0.0187983401119709
step: 100 , loss: 0.003726101480424404
0.004024248104542494
step: 110 , loss: 0.002330860123038292
0.004556658677756786
step: 120 , loss: 0.005564437247812748
0.004920314531773329
step: 130 , loss: 0.0034740460105240345
0.003842004109174013
step: 140 , loss: 0.006782074924558401
0.00554296188056469
step: 150 , loss: 0.004529284778982401
0.0041749440133571625
step: 160 , loss: 0.005463242530822754
0.02945753000676632
step: 170 , loss: 0.005143584683537483
0.005622758064419031
step: 180 , loss: 0.0025210150051862
0.004917480051517487
step: 190 , loss: 0.002093826187774539
0.003613394685089588
step: 200 , loss: 0.006344197317957878
0.004035523626953363
step: 210 , loss: 0.0064163520000875
0.004805918782949448
step: 220 , loss: 0.002519287634640932
0.003895755158737302
step: 230 , loss: 0.004286103416234255
0.003922672010958195
step: 240 , loss: 0.004982600919902325
0.0038582603447139263
step: 250 , loss: 0.005200282204896212
0.006197696551680565
step: 260 , loss: 0.004439763259142637
0.0038322110194712877
step: 270 , loss: 0.004021055996417999
0.003938755020499229
step: 280 , loss: 0.006458204705268145
0.006083773449063301
step: 290 , loss: 0.004716489464044571
0.0041619255207479
step: 300 , loss: 0.00645638071000576
0.004229363054037094
step: 310 , loss: 0.00485848868265748
0.004736597184091806
step: 320 , loss: 0.006153483409434557
0.0037834204267710447
step: 330 , loss: 0.0025016898289322853
0.003990678582340479
step: 340 , loss: 0.004913853015750647
0.009565344080328941
step: 350 , loss: 0.005234694108366966
0.0061312164179980755
step: 360 , loss: 0.005791701376438141
0.0034137528855353594
step: 370 , loss: 0.00291990558616817
0.0036837211810052395
step: 380 , loss: 0.0029731381218880415
0.0037761935964226723
step: 390 , loss: 0.005085727199912071
0.003914740867912769
step: 400 , loss: 0.0034714853391051292
0.006418275646865368
step: 410 , loss: 0.0008412152528762817
0.001747959409840405
step: 420 , loss: 0.002190102357417345
0.001145644928328693
step: 430 , loss: 0.0008639618754386902
0.0013805205235257745
step: 440 , loss: 0.000960043806117028
0.0009299575467593968
step: 450 , loss: 0.0006482850876636803
0.0009270022856071591
step: 460 , loss: 0.0005585501785390079
0.0007245164015330374
step: 470 , loss: 0.0006825503660365939
0.0007931942236609757
step: 480 , loss: 0.002197416964918375
0.0024914583191275597
step: 490 , loss: 0.0007055035675875843
0.000482806412037462
step: 500 , loss: 0.0006847624899819493
0.000544509501196444
lr: 0.005
activation function type: relu
depth: 2
width: 20
test_loss: 0.0005516210221685469
