step: 10 , loss: 0.5068504810333252
0.3898877799510956
step: 20 , loss: 0.324529767036438
0.31980958580970764
step: 30 , loss: 0.2266373336315155
0.240346297621727
step: 40 , loss: 0.17018729448318481
0.1765041947364807
step: 50 , loss: 0.10429316759109497
0.13123886287212372
step: 60 , loss: 0.11119304597377777
0.09770406037569046
step: 70 , loss: 0.06408895552158356
0.07917983084917068
step: 80 , loss: 0.018860964104533195
0.0695958137512207
step: 90 , loss: 0.07231496274471283
0.06354554742574692
step: 100 , loss: 0.06763007491827011
0.06014483794569969
step: 110 , loss: 0.0806831419467926
0.05993840843439102
step: 120 , loss: 0.07250533252954483
0.057119302451610565
step: 130 , loss: 0.06645062565803528
0.05555472895503044
step: 140 , loss: 0.09216156601905823
0.051978904753923416
step: 150 , loss: 0.04661976546049118
0.05021677538752556
step: 160 , loss: 0.04268969967961311
0.049604374915361404
step: 170 , loss: 0.018344735726714134
0.04796139523386955
step: 180 , loss: 0.06356972455978394
0.04729903116822243
step: 190 , loss: 0.02029128558933735
0.044939618557691574
step: 200 , loss: 0.026455821469426155
0.042430128902196884
step: 210 , loss: 0.02665916085243225
0.040988244116306305
step: 220 , loss: 0.035758014768362045
0.03988688811659813
step: 230 , loss: 0.025995463132858276
0.03790046274662018
step: 240 , loss: 0.007471323944628239
0.036481235176324844
step: 250 , loss: 0.02147195115685463
0.03552170470356941
step: 260 , loss: 0.055674657225608826
0.033364877104759216
step: 270 , loss: 0.05710621550679207
0.0317281149327755
step: 280 , loss: 0.023779068142175674
0.03028312884271145
step: 290 , loss: 0.0216009933501482
0.028660306707024574
step: 300 , loss: 0.017612531781196594
0.027161719277501106
step: 310 , loss: 0.02968648262321949
0.02568165212869644
step: 320 , loss: 0.010333791375160217
0.024066388607025146
step: 330 , loss: 0.009530216455459595
0.02253604494035244
step: 340 , loss: 0.00956086628139019
0.019442647695541382
step: 350 , loss: 0.03552009165287018
0.016289612278342247
step: 360 , loss: 0.010889211669564247
0.014760521240532398
step: 370 , loss: 0.009268861263990402
0.013116213493049145
step: 380 , loss: 0.01215385738760233
0.01208821777254343
step: 390 , loss: 0.018295269459486008
0.010795330628752708
step: 400 , loss: 0.00773515785112977
0.009300136007368565
step: 410 , loss: 0.007080503739416599
0.008699373342096806
step: 420 , loss: 0.002911068033427
0.007519247476011515
step: 430 , loss: 0.001858251984231174
0.006561670918017626
step: 440 , loss: 0.008718283846974373
0.005726935807615519
step: 450 , loss: 0.004676968324929476
0.005113809369504452
step: 460 , loss: 0.004672281444072723
0.004999477416276932
step: 470 , loss: 0.0028565640095621347
0.004130878485739231
step: 480 , loss: 0.0024808624293655157
0.0038001176435500383
step: 490 , loss: 0.0021105357445776463
0.0032537924125790596
step: 500 , loss: 0.001362676965072751
0.002946286927908659
lr: 0.001
activation function type: relu
depth: 2
width: 20
test_loss: 0.0032707934733480215
