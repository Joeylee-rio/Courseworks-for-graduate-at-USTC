0.002513776878247484
(tensor([6.5132], device='cuda:0'), tensor([0.2295], device='cuda:0'))
3500
Net(
  (inp_layer): Linear(in_features=1, out_features=20, bias=True)
  (hiddens): ModuleList(
    (0): Linear(in_features=20, out_features=20, bias=True)
  )
  (out_layer): Linear(in_features=20, out_features=1, bias=True)
)
step: 10 , loss: 0.4157484471797943
0.3368876278400421
step: 20 , loss: 0.31285732984542847
0.3372225761413574
step: 30 , loss: 0.32123756408691406
0.32348206639289856
step: 40 , loss: 0.32000815868377686
0.3245488107204437
step: 50 , loss: 0.2632136344909668
0.3233254849910736
step: 60 , loss: 0.36526569724082947
0.3296205401420593
step: 70 , loss: 0.32111069560050964
0.3262399733066559
step: 80 , loss: 0.413405179977417
0.3236188292503357
step: 90 , loss: 0.29416728019714355
0.325489342212677
step: 100 , loss: 0.33171042799949646
0.3236003518104553
step: 110 , loss: 0.2733100652694702
0.3239845633506775
step: 120 , loss: 0.28673070669174194
0.3029955327510834
step: 130 , loss: 0.09994029998779297
0.15240144729614258
step: 140 , loss: 0.05754215270280838
0.06417243182659149
step: 150 , loss: 0.036167874932289124
0.04321031644940376
step: 160 , loss: 0.020329782739281654
0.03779450058937073
step: 170 , loss: 0.01322647463530302
0.03408452868461609
step: 180 , loss: 0.028181079775094986
0.029338615015149117
step: 190 , loss: 0.01478774193674326
0.023684121668338776
step: 200 , loss: 0.007710454519838095
0.021993601694703102
step: 210 , loss: 0.01890900731086731
0.015299717895686626
step: 220 , loss: 0.009835144504904747
0.012548979371786118
step: 230 , loss: 0.007787447422742844
0.009408963844180107
step: 240 , loss: 0.006636724807322025
0.005931793712079525
step: 250 , loss: 0.005129604134708643
0.003949700854718685
step: 260 , loss: 0.0017055949429050088
0.003197350772097707
step: 270 , loss: 0.0010124539257958531
0.002705022692680359
step: 280 , loss: 0.002430056454613805
0.001655364641919732
step: 290 , loss: 0.000415668822824955
0.0014267755905166268
step: 300 , loss: 0.000693583395332098
0.0010793438414111733
step: 310 , loss: 0.0009881608420982957
0.0010556899942457676
step: 320 , loss: 0.0012373370118439198
0.0007254309603013098
step: 330 , loss: 0.00032905611442402005
0.0005628828657791018
step: 340 , loss: 0.0005116614047437906
0.0009008434717543423
step: 350 , loss: 0.0004017877799924463
0.000366145366569981
step: 360 , loss: 0.00046585052041336894
0.0003788014582823962
step: 370 , loss: 0.0002413676556898281
0.0003540367179084569
step: 380 , loss: 0.00034371597575955093
0.0004884632653556764
step: 390 , loss: 0.0004359001759439707
0.0002821148664224893
step: 400 , loss: 0.0003158174513373524
0.0003614361921790987
step: 410 , loss: 0.00033983655157499015
0.000322534644510597
step: 420 , loss: 0.00036183474003337324
0.0002553283702582121
step: 430 , loss: 0.0004076059558428824
0.00018506620835978538
step: 440 , loss: 0.00023087106819730252
0.00047214736696332693
step: 450 , loss: 0.0003154915466438979
0.00019838997104670852
step: 460 , loss: 0.0004036405880469829
0.00042045957525260746
step: 470 , loss: 0.0005013479967601597
0.00045900430995970964
step: 480 , loss: 0.00032070939778350294
0.000302128930343315
step: 490 , loss: 0.0002359873615205288
0.00023214705288410187
step: 500 , loss: 0.0002842480898834765
0.00028119757189415395
lr: 0.001
activation function type: relu
depth: 2
width: 20
test_loss: 0.000306094967527315
